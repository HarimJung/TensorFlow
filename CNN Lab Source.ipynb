{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# CNN\n",
    "이 노트는 CNN(Convolution Neural Network)을 배우기 위한 주피터노트북이다. 먼저, 이 CNN이 영상에 잘 적용되는 이유를 설명하고자 한다. 한 가지 일관되게 기억해야 할 가장 중요한 사실은 CNN은 함수이며 함수는 입력과 출력을 연결짓는 블랙박스(미지의 어떤 것)라는 점이다. 그래서, CNN은 입력: 영상(image),\n",
    "출력: 범주(nominal)이다. 범주는 고양이, 호랑이와 같은 개념들이고 보면 되고, 그렇기 때문에 영상을 개념들로 맵핑한다 하여 CNN을 분류기라고도 부른다.\n",
    "\n",
    "먼저 CNN을 이해하기 위해 입력으로 들어올 영상을 알아보자. 이를 위해 파이썬 코드에서 이미지(정지영상)의 표현법을 익혀보도록 하자.\n",
    "\n",
    "\n",
    "## 정지영상(이미지)\n",
    "이미지는 파이썬 코드에서 흑백영상의 경우 1차원 배열로, 컬러영상의 경우 3차원 배열로 표현된다.\n",
    "\n",
    "예를 들어, 28x28 크기의 흑백영상은 (28,28,1)로 표현되며, 컬러영상은 (28, 28, 3)으로 표현된다. 즉, 3차원배열의 마지막 차원이 흑백의 경우 gray scale의 1채널이 존재하고, 컬러영상의 경우 R,G,B scale의 3채널이 존재한다. 1,2차원은 영상의 가로, 세로 크기를 의미한다. \n",
    "\n",
    "여기서, numpy의 array를 이용한 간단한 영상 표현법을 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 3, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13b43a4a8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#tensorflow interactive session\n",
    "sess = tf.InteractiveSession()\n",
    "image = np.array([[[[1],[2],[3]],\n",
    "                 [[4],[5],[6]],\n",
    "                             [[7],[8],[9]]]],dtype=np.float32)\n",
    "print(image.shape)\n",
    "plt.imshow(image.reshape(3,3),cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[1.  0.  0. ]\n",
      "   [0.  1.  0. ]\n",
      "   [0.  0.  1. ]]\n",
      "\n",
      "  [[1.  1.  0. ]\n",
      "   [0.  1.  1. ]\n",
      "   [1.  0.  1. ]]\n",
      "\n",
      "  [[1.  0.5 0.5]\n",
      "   [0.5 1.  0.5]\n",
      "   [0.5 0.5 1. ]]]]\n",
      "(1, 3, 3, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13bbaf3c8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#tensorflow interactive session\n",
    "sess = tf.InteractiveSession()\n",
    "image = np.array([[[[1.,0.,0.], [0.,1.,0.], [0.,0.,1.]],\n",
    "                  [[1.,1.,0.], [0.,1.,1.], [1.,0.,1.]],\n",
    "                  [[1.,.5,.5], [.5,1.,.5], [.5,.5,1.]]]],dtype=np.float32)\n",
    "print(image)\n",
    "print(image.shape)\n",
    "plt.imshow(image[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 numpy array에 대해서 좀더 상세하게 알고 싶다면, 다음 링크 https://jakevdp.github.io/PythonDataScienceHandbook/02.02-the-basics-of-numpy-arrays.html\n",
    "참조하라. \n",
    "간단하게 요점만 언급하면, array에서 꼭 알아야 할 사항은 ndim, shape, size의 개념을 구분할 줄 알아야 한다는 점이다.\n",
    "그리고, 이것을 1차원 array, 2차원 array, 3차원 array, 혹은 그 이상 차원의  array 표현 방법을 익혀두면 앞으로 영상데이터를 다루는 데 꼭 필요한 선결조건이다. 이 기초가 튼튼하지 못하면 파이썬 코드를 이해하는 데 어려움을 격게 될 것이다.\n",
    "\n",
    "##filter\n",
    "\n",
    "아래 코드는 tensorflow의 constant로 weight를 정의한 것이다. 여기서, array형태의 데이터가 인자로 전달된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight.shape:  (2, 2, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "weight=tf.constant([[[[1.]],[[1.]]],\n",
    "                    [[[1.]],[[1.]]]])\n",
    "print(\"weight.shape: \",weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서, array는 (2,2,1,1)의 shape을 가진다. 4차원 배열이다. 수강생 여러분은 여기서 한가지 확실해하고 넘어가야할 부분은 \"어떤 shape의 array든지 정의할 수 있도록\" 연습을 해야 한다. 이렇게 정의할 수 있도록 하는 좋은 한가지 팁은 [다음의 [가 몇 개 나오는지가 shape의 모양을 결정한다. 단, 주의할 사항은 제일 마지막에는 []안의 원소의 개수로 정해진다. 위의 예에서, 대괄호([)가 4개 나온다. 4차원 배열이다. 다음으로 각 차원에서 어떤 shape을 가지느냐 이다.  그것은 뒤에서 세는 것이 좋다. 즉, 제일 안쪽 대괄호의 안의 원소를 꼼마로 구분할 떄 몇 개인지가 제일 마지막 차원의 크기이다. 구분없이 1개 존재한다. 다음으로 제일 안쪽 대괄호를 포함하는 바깥 대괄호의 원소를 콤마로 구분하여 몇 개 있느냐이다. 역시, 1개이다. 그 다음 바깥 대괄호는 2개의 원소를 가진다. 또한, 다음 바깥(사실상 제일 바깥) 대괄호는 역시 2개를 가진다. 따라서, 2,2,1,1이 된다.\n",
    "\n",
    "\n",
    "## Tensorflow에서 이미지배열과 필터배열의 각 차원의 의미\n",
    "Tensorflow에서의 이미지배열과 필터배열에 대한 의미는 다음과 같이 약속한다.\n",
    "\n",
    "Image: 1,3,3,3 image, Filter: 2,2,3,1, Stride: 1x1x1x1, Padding: VALID\n",
    "1,3,3,3 image: (image개수, 가로픽셀, 세로픽셀, 색깔채널수)\n",
    "\n",
    "2,2,3,1 filter: (가로픽셀,세로픽셀,색깔채널수, 필터개수)\n",
    "\n",
    "1,1 stride: (가로, 세로)\n",
    "\n",
    "valid\n",
    "\n",
    "SAME: 입력영상의 크기와 동일한 결과를 만들 수 있도록 바깥부분을 0으로 채운다.\n",
    "\n",
    "## Convolution Layer적용\n",
    "이제 이미지배열도 준비되었고, 필터배열도 준비되었으므로 convolution layer를 만들어 보자.\n",
    "\n",
    "convolution layer는 tensorflow의 nn모듈에 포함된 conv2d함수를 이용한다. 함수의 인자는 다음과 같다.\n",
    "\n",
    "tf.nn.conv2d(\n",
    "image,\n",
    "filter, \n",
    "strides,\n",
    "padding)\n",
    "\n",
    "위의 함수가 의도한대로 동작할 수 있도록 적절한 인자를 작성하여 넘겨준다.\n",
    "1) image -> 앞에서 살펴보았음\n",
    "2) filter -> 앞에서 살펴보았음\n",
    "3) strides -> 앞에서 살펴보았음. 1,2,4차원배열로 표현 가능. 1차원은 가로세로 동일, 2차원은 가로, 세로를 의미한다. 4차원은 (이미지,가로,세로,채널)의미\n",
    "4) padding -> valid, same 중의 하나를 선택함\n",
    "\n",
    "실제 코드는 아래와 같다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape (1, 3, 3, 3)\n",
      "weight.shape:  (2, 2, 3, 1)\n",
      "conv2d_image.shape:  (1, 3, 3, 1)\n",
      "[[6. 6. 3.]\n",
      " [8. 8. 4.]\n",
      " [4. 4. 2.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIEAAACFCAYAAAB8MZtGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAABDVJREFUeJzt3bFrnHUcx/H3x2oh1BsCOoS2aIVi6GYJhSI4OITq0qFLO7hUyFRQcOnYv8DNJWBwEYugQ4dCcRBEKtJYFNqGSixIUwS1FnQolMLXoRmiiPdI7/k9F/N+QSB3OZ77Et489+Ryv+dJVaGd7YmhB9DwjEBGICMQRiCMQBiBMAJhBAKe7GOjMzMzNRqN+tj0tjI3Nzfo829sbHD37t2Me1wvEYxGI06cONHHpreVc+fODfr8i4uLnR7ny4GMQEYgjEAYgTACYQTCCIQRCCMQRiA6RpDkWJKbSdaTnO17KLU1NoIku4D3gNeAQ8CpJIf6HkztdNkTHAHWq+pWVT0AzgPH+x1LLXWJYC9we8vtjc379D8xsQPDJEtJVpOs3r9/f1KbVQNdIrgD7N9ye9/mfX9RVctVtVBVCzMzM5OaTw10ieAKcDDJgSS7gZPAhX7HUktjP15WVQ+TnAEuAbuAlaq63vtkaqbTZwyr6iJwsedZNBDfMZQRyAiEEQgjEEYgjEAYgTACYQTCCASkj9PaJvFcucDly5cHff7Tp0+ztrY29iQV7glkBDICYQTCCIQRCCMQRiCMQBiBMAJhBKLb+QlWkvyc5FqLgdRelz3BB8CxnufQgMZGUFVfAL81mEUD8ZhAk7voRZIlYGlS21M7E4ugqpaBZfCTRduNLwfq9CfiR8BXwItJNpK82f9YaqnLmUpOtRhEw/HlQEYgIxBGIIxAGIEwAmEEwgiEEQgjEBP8V/JW8/PzrKys9LHpbeXo0aODPv+ePXs6Pc49gYxARiCMQBiBMAJhBMIIhBEIIxBGIIxAdFuBtD/J50luJLme5K0Wg6mdLv9FfAi8U1VXk4yAb5J8VlU3ep5NjXQ5ScVPVXV18/s/gDVgb9+DqZ3/dEyQ5HngJeDrf/jZUpLVJKv37t2bzHRqonMESZ4GPgHerqrf//7zqlquqoWqWpidnZ3kjOpZpwiSPMWjAD6sqk/7HUmtdfnrIMD7wFpVvdv/SGqty57gZeAN4NUk325+vd7zXGqoy0kqvgTGXlFL25fvGMoIZATCCIQRCCMQRiCMQBiBMAJhBAJSNflLEyT5BfjxMTbxDPDrhMbZyTM8V1XPjntQLxE8riSrVbXgDG1m8OVARqDpjWB56AHYQTNM5TGB2prWPYEamqoIkhxLcjPJepKzA80w6LWhB1n2V1VT8QXsAn4AXgB2A98BhwaY4xXgMHBtoN/DHHB48/sR8H3fv4dp2hMcAdar6lZVPQDOA8dbD1EDXxu6Blj2N00R7AVub7m9wQ5f8/hvy/4maZoi0Bbjlv1N0jRFcAfYv+X2vs37dpzWy/6mKYIrwMEkB5LsBk4CFwaeqbkhlv1NTQRV9RA4A1zi0cHQx1V1vfUcU3Bt6ObL/nzHUNOzJ9BwjEBGICMQRiCMQBiBMAIBfwIADGixyV5yLQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"image.shape\", image.shape)\n",
    "weight=tf.constant([[[[1.],[1.],[1.]],[[1.],[1.],[1.]]],\n",
    "                    [[[1.],[1.],[1.]],[[1.],[1.],[1.]]]])\n",
    "print(\"weight.shape: \",weight.shape)\n",
    "\n",
    "conv2d=tf.nn.conv2d(image, weight, strides=[1,1,1,1], padding=\"SAME\")\n",
    "conv2d_img=conv2d.eval()\n",
    "\n",
    "#shape으로 확인하기 위한 코드\n",
    "print(\"conv2d_image.shape: \", conv2d_img.shape)\n",
    "\n",
    "# 시각화하기 위한 코드 \n",
    "conv2d_img=np.swapaxes(conv2d_img,0,3)\n",
    "for i, one_img in enumerate(conv2d_img):\n",
    "    print(one_img.reshape(3,3))\n",
    "    plt.subplot(1,3,i+1),    plt.imshow(one_img[:,:,0].reshape(3,3),cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 filter (2,2,3,3): (가로, 세로, 색채널개수, 필터 개수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape (1, 3, 3, 3)\n",
      "weight.shape:  (2, 2, 3, 3)\n",
      "conv2d_image.shape:  (1, 3, 3, 3)\n",
      "[[6. 6. 3.]\n",
      " [8. 8. 4.]\n",
      " [4. 4. 2.]]\n",
      "[[60. 60. 30.]\n",
      " [80. 80. 40.]\n",
      " [40. 40. 20.]]\n",
      "[[-6. -6. -3.]\n",
      " [-8. -8. -4.]\n",
      " [-4. -4. -2.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"image.shape\", image.shape)\n",
    "weight=tf.constant([[[[1.,10.,-1.],[1.,10.,-1.],[1.,10.,-1.]],[[1.,10.,-1.],[1.,10.,-1.],[1.,10.,-1.]]],\n",
    "                    [[[1.,10.,-1.],[1.,10.,-1.],[1.,10.,-1.]],[[1.,10.,-1.],[1.,10.,-1.],[1.,10.,-1.]]]])\n",
    "print(\"weight.shape: \",weight.shape)\n",
    "conv2d=tf.nn.conv2d(image, weight, strides=[1,1,1,1], padding=\"SAME\")\n",
    "conv2d_img=conv2d.eval()\n",
    "\n",
    "print(\"conv2d_image.shape: \", conv2d_img.shape)\n",
    "conv2d_img=np.swapaxes(conv2d_img,0,3)\n",
    "for i, one_img in enumerate(conv2d_img):\n",
    "    print(one_img.reshape(3,3))\n",
    "    plt.subplot(1,3,i+1),    plt.imshow(one_img[:,:,0].reshape(3,3),cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Pooling\n",
    "\n",
    "pooling의 사전적 의미는 관계되는 것들 간의 공유를 의미한다. max pooling은 공유를 통하여 최대값을 추출하는 것이라고 볼 수 있다. Max pooling은 앞의 convolution layer의 filter와 유사한 문법을 가진다. 아래의 내용을 보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "image = np.array([\n",
    "                  [\n",
    "                    [[4.],[3.]],\n",
    "                    [[2.],[1.]]\n",
    "                  ]\n",
    "                 ],dtype=np.float32)\n",
    "image_mp=tf.nn.max_pool(image, ksize=[1,2,2,1],strides=[1, 1, 1, 1],\n",
    "                        padding=\"SAME\")\n",
    "print(image_mp.shape)\n",
    "print(image_mp.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 max_pool의 인자는 convolution layer를 거친 image이다. ksize는 필터사이즈를 의미하고 strides는 이동간격을 의미한다. padding에서 \"SAME\"은 image size 와 같은 결과를 내놓기 위해 0-padding한다.\n",
    "\n",
    "ksize는 일반적으로 [1,2,2,1]을 사용한다. 이것의 의미는 2x2사이즈의 필터를 의미한다. 여기서, 각 원소의 의미는 [image, width, hight, channel]다. 예를 들어, [2, 5, 5, 3] 의 의미는 2개의 이미지 중 가로x세로, 5x5의 필터를 써서 3채널 중 가장 큰 값을 취한다이다.  \n",
    "\n",
    "여기서는 strides가 1씩이기 때문에 같은 결과가 나온다. 만약 strides size를 변화시키면 어떤 변화가 생기는지 확인해 보라. \n",
    "\n",
    "##Strides\n",
    "strides=[1,2,1,1] #가로로 2씩 이동하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 2, 1)\n",
      "[[[[4.]\n",
      "   [3.]]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "image = np.array([\n",
    "                  [\n",
    "                    [[4.],[3.]],\n",
    "                    [[2.],[1.]]\n",
    "                  ]\n",
    "                 ],dtype=np.float32)\n",
    "image_mp=tf.nn.max_pool(image, ksize=[1,2,2,1],strides=[1, 2, 1, 1],\n",
    "                        padding=\"SAME\")\n",
    "print(image_mp.shape)\n",
    "print(image_mp.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "strides=[1,1,2,1] #세로로 2씩 이동하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 1, 1)\n",
      "[[[[4.]]\n",
      "\n",
      "  [[2.]]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "image = np.array([\n",
    "                  [\n",
    "                    [[4.],[3.]],\n",
    "                    [[2.],[1.]]\n",
    "                  ]\n",
    "                 ],dtype=np.float32)\n",
    "image_mp=tf.nn.max_pool(image, ksize=[1,2,2,1],strides=[1, 1, 2, 1],\n",
    "                        padding=\"SAME\")\n",
    "print(image_mp.shape)\n",
    "print(image_mp.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max pooling의 동작원리\n",
    "\n",
    "원래 Max Pooling은 size를 줄이는 효과가 있다. 그래서, Strides를 [1,n1,n2,1]로 하면, 가로크기가 1/n1로, 세로크기가 1/n2로 줄어든다.\n",
    "물론, ksize도 [1,n1,n2,1]로 맞추어주는 것이 일반적이다.\n",
    "\n",
    "아래 예의 경우, 이미지가 1,2,3,4로 바꾸었는데 결과는 모두 4로 나온다. 이는 max pooling의 오류를 만들 수 있다는 점이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 2, 1)\n",
      "[[[[4.]\n",
      "   [4.]]\n",
      "\n",
      "  [[4.]\n",
      "   [4.]]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "image = np.array([[[[1.],[2.]],\n",
    "                   [[3.],[4.]]]],dtype=np.float32)\n",
    "image_mp=tf.nn.max_pool(image, ksize=[1,2,2,1],strides=[1, 1, 1, 1],\n",
    "                        padding=\"SAME\")\n",
    "print(image_mp.shape)\n",
    "print(image_mp.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN으로 MNIST DATA 분류하기\n",
    "## MNIST DATA 불러오기\n",
    "MNIST data는 손글씨 이미지 데이터세트이다. 얀 르쿤, 코리나 콜테츠, 버그즈에 의해 작성되었다. (http://yann.lecun.com/exdb/mnist/ 참조)\n",
    "tensorflow에서 손쉽게 불러오기하여 작성한 모델의 성능을 테스트할 수 있다.\n",
    "아래는 mnist를 활용하는 예를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train),(x_test,y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14ae869e8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img0 = x_train[0].reshape(28,28)\n",
    "plt.imshow(img0, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Conv2D_27:0\", shape=(1, 14, 14, 6), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "img = tf.cast(tf.reshape(img,[-1,28,28,1]),tf.float32)\n",
    "#print(img)\n",
    "#img=tf.cast(img, tf.float32)\n",
    "#print(img)\n",
    "# W1 weight 즉, filter를 설정해라.\n",
    "# 초기화가 중요하다. 초기화에서 초기치 설정방법과 크기가 중요하다. \n",
    "# 크기의 의미는 (필터 가로크기, 필터 세로크기, 색깔 channel, 필터개수)\n",
    "# 초기화는 표준편차 0.1의 정규분포(random)로 생성하라.\n",
    "W1=tf.Variable(tf.random_normal([3,3,1,6], stddev=0.03))\n",
    "#conv2d의 인자의미는 (conv할 이미지, filter, strides, padding)\n",
    "conv_layer = tf.nn.conv2d(img,W1,strides=[1,2,2,1], padding=\"SAME\")\n",
    "print(conv_layer)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "conved_img = conv_layer.eval()\n",
    "conved_img = np.swapaxes(conved_img,0,3)\n",
    "for i, one_img in enumerate(conved_img):\n",
    "    plt.subplot(1,6,i+1), plt.imshow(one_img.reshape(14,14), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MaxPool_19:0\", shape=(1, 7, 7, 6), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#MNIST Max pooling\n",
    "# max_pool 함수의 인자의미는 (입력이미지, max_pool필터사이즈, strides, padding)\n",
    "conved_img=conved_img.reshape(-1,14,14,6)\n",
    "max_pool_layer = tf.nn.max_pool(conved_img, ksize=[1,2,2,1], \n",
    "                            strides=[1,2,2,1], padding='VALID')\n",
    "print(max_pool_layer)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "max_pooled_img = max_pool_layer.eval()\n",
    "max_pooled_img = np.swapaxes(max_pooled_img, 0, 3)\n",
    "for i, one_img in enumerate(max_pooled_img):\n",
    "    plt.subplot(1,6, i+1), plt.imshow(one_img.reshape(7,7), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#from tensorflow.keras.datasets import mnist\n",
    "#(x_train, y_train),(x_test,y_test) = mnist.load_data()\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "#batch_xs, bat_ys = mnist.train.next_batch(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 본격적으로 CNN으로 MNIST데이터 분류하기\n",
    "\n",
    "### CNN의 구조\n",
    " 입력자료는 [None, 28, 28, 1]이다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.279568226\n",
      "Epoch: 0002 cost = 0.086532600\n",
      "Epoch: 0003 cost = 0.061857335\n",
      "Epoch: 0004 cost = 0.048856902\n",
      "Epoch: 0005 cost = 0.041343786\n",
      "Epoch: 0006 cost = 0.034895854\n",
      "Epoch: 0007 cost = 0.028989839\n",
      "Epoch: 0008 cost = 0.025596548\n",
      "Epoch: 0009 cost = 0.021955364\n",
      "Epoch: 0010 cost = 0.019150648\n",
      "Epoch: 0011 cost = 0.016451509\n",
      "Epoch: 0012 cost = 0.013961930\n",
      "Epoch: 0013 cost = 0.012442831\n",
      "Epoch: 0014 cost = 0.011642529\n",
      "Epoch: 0015 cost = 0.008668751\n",
      "Learning Finished!\n",
      "Accuracy: 0.9849\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#input placeholders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "X_img = tf.reshape(X, [-1, 28, 28, 1])\n",
    "\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "#Conv L1 ImgIn shape=(?, 28, 28, 1)\n",
    "# Filter W1 size=[3,3,1,6]\n",
    "W1 = tf.Variable(tf.random_normal([3,3,1, 32],stddev=0.03))\n",
    "\n",
    "#L1\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides=[1,1,1,1], padding='SAME')\n",
    "L1_img = tf.nn.relu(L1)\n",
    "\n",
    "#Max pooling MP1 size \n",
    "MP1 = tf.nn.max_pool(L1_img, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "#Conv L2 ImgIn shape\n",
    "#Filter W2 size\n",
    "W2 = tf.Variable(tf.random_normal([3,3,32, 64],stddev=0.03))\n",
    "\n",
    "#L2\n",
    "L2 = tf.nn.conv2d(MP1, W2, strides=[1,1,1,1], padding='SAME')\n",
    "L2_img = tf.nn.relu(L2)\n",
    "\n",
    "#MP2\n",
    "MP2 = tf.nn.max_pool(L2_img, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "MP2 = tf.reshape(MP2, [-1, 7 * 7 * 64])\n",
    "\n",
    "W3=tf.Variable(tf.random_normal([7*7*64,10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "#몇몇 곳에서는 prediction 혹은 result라고 하기도 한다. hypothesis는 가설로 번역될 수 있고, 예상되는 값이라는 의미이다.\n",
    "hypothesis = tf.matmul(MP2,W3)+b\n",
    "learning_rate = 0.001\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "#Training and Evaluation\n",
    "#initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "for epoch in range(training_epochs):\n",
    "    average_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _, = sess.run ([cost, optimizer], feed_dict=feed_dict)\n",
    "        average_cost += c /total_batch\n",
    "    print('Epoch:', \"%04d\" % (epoch + 1), 'cost =', '{:.9f}'.format(average_cost))\n",
    "                \n",
    "print('Learning Finished!')\n",
    "                \n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Input to reshape is a tensor with 313600 values, but the requested shape requires a multiple of 2048\n\t [[node Reshape_38 (defined at <ipython-input-37-b12e1d349faf>:41) ]]\n\nCaused by op 'Reshape_38', defined at:\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py\", line 539, in run_forever\n    self._run_once()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py\", line 1775, in _run_once\n    handle._run()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2843, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2869, in _run_cell\n    return runner(coro)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3044, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3209, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3291, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-37-b12e1d349faf>\", line 41, in <module>\n    MP3 = tf.reshape(MP3, [-1, 4 * 4 * 128])\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 7179, in reshape\n    \"Reshape\", tensor=tensor, shape=shape, name=name)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Input to reshape is a tensor with 313600 values, but the requested shape requires a multiple of 2048\n\t [[node Reshape_38 (defined at <ipython-input-37-b12e1d349faf>:41) ]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Input to reshape is a tensor with 313600 values, but the requested shape requires a multiple of 2048\n\t [[{{node Reshape_38}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-b12e1d349faf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0maverage_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%04d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cost ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{:.9f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_cost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Input to reshape is a tensor with 313600 values, but the requested shape requires a multiple of 2048\n\t [[node Reshape_38 (defined at <ipython-input-37-b12e1d349faf>:41) ]]\n\nCaused by op 'Reshape_38', defined at:\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py\", line 539, in run_forever\n    self._run_once()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py\", line 1775, in _run_once\n    handle._run()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2843, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2869, in _run_cell\n    return runner(coro)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3044, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3209, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3291, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-37-b12e1d349faf>\", line 41, in <module>\n    MP3 = tf.reshape(MP3, [-1, 4 * 4 * 128])\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 7179, in reshape\n    \"Reshape\", tensor=tensor, shape=shape, name=name)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Input to reshape is a tensor with 313600 values, but the requested shape requires a multiple of 2048\n\t [[node Reshape_38 (defined at <ipython-input-37-b12e1d349faf>:41) ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#input placeholders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "X_img = tf.reshape(X, [-1, 28, 28, 1])\n",
    "\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "#Conv L1 ImgIn shape=(?, 28, 28, 1)\n",
    "# Filter W1 size=[3,3,1,6]\n",
    "W1 = tf.Variable(tf.random_normal([3,3,1, 32],stddev=0.03))\n",
    "\n",
    "#L1\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides=[1,1,1,1], padding='SAME')\n",
    "L1_img = tf.nn.relu(L1)\n",
    "\n",
    "#Max pooling MP1 size \n",
    "MP1 = tf.nn.max_pool(L1_img, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "#Conv L2 ImgIn shape\n",
    "#Filter W2 size\n",
    "W2 = tf.Variable(tf.random_normal([3,3,32, 64],stddev=0.03))\n",
    "\n",
    "#L2\n",
    "L2 = tf.nn.conv2d(MP1, W2, strides=[1,1,1,1], padding='SAME')\n",
    "L2_img = tf.nn.relu(L2)\n",
    "\n",
    "#MP2\n",
    "MP2 = tf.nn.max_pool(L2_img, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "MP2 = tf.reshape(MP2, [-1, 7 * 7 * 64])\n",
    "\n",
    "#Conv L3 ImgIn shape\n",
    "#Filter W3 size\n",
    "W3 = tf.Variable(tf.random_normal([3,3,64, 128],stddev=0.03))\n",
    "\n",
    "#L3\n",
    "L3 = tf.nn.conv2d(MP1, W2, strides=[1,1,1,1], padding='SAME')\n",
    "L3_img = tf.nn.relu(L3)\n",
    "\n",
    "#MP3\n",
    "MP3 = tf.nn.max_pool(L3_img, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "MP3 = tf.reshape(MP3, [-1, 4 * 4 * 128])\n",
    "\n",
    "W4=tf.Variable(tf.random_normal([4*4*128,10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(MP3,W4)+b\n",
    "learning_rate = 0.001\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "#Training and Evaluation\n",
    "#initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "for epoch in range(training_epochs):\n",
    "    average_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _, = sess.run ([cost, optimizer], feed_dict=feed_dict)\n",
    "        average_cost += c /total_batch\n",
    "    print('Epoch:', \"%04d\" % (epoch + 1), 'cost =', '{:.9f}'.format(average_cost))\n",
    "                \n",
    "print('Learning Finished!')\n",
    "                \n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Load an color image in grayscale\n",
    "img = cv2.imread('messi5.jpg',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[1.  0.  0. ]\n",
      "   [0.  1.  0. ]\n",
      "   [0.  0.  1. ]]\n",
      "\n",
      "  [[1.  1.  0. ]\n",
      "   [0.  1.  1. ]\n",
      "   [1.  0.  1. ]]\n",
      "\n",
      "  [[1.  0.5 0.5]\n",
      "   [0.5 1.  0.5]\n",
      "   [0.5 0.5 1. ]]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0, 1, 2],\n",
       "        [0, 1, 0],\n",
       "        [0, 1, 2]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(image)\n",
    "sess.run(tf.argmax(image,0))\n",
    "sess.run(tf.argmax(image,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras를 이용한 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 11.6268 - acc: 0.2780 - val_loss: 11.2812 - val_acc: 0.3000\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 86s 1ms/step - loss: 12.0988 - acc: 0.2493 - val_loss: 13.0847 - val_acc: 0.1882\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 91s 2ms/step - loss: 12.8314 - acc: 0.2039 - val_loss: 11.7162 - val_acc: 0.2731\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 92s 2ms/step - loss: 11.5084 - acc: 0.2860 - val_loss: 11.5099 - val_acc: 0.2859\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 92s 2ms/step - loss: 12.0329 - acc: 0.2534 - val_loss: 11.7388 - val_acc: 0.2717\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 11.5091 - acc: 0.2859 - val_loss: 11.8678 - val_acc: 0.2637\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 11.3357 - acc: 0.2967 - val_loss: 11.2601 - val_acc: 0.3014\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 10.7200 - acc: 0.3349 - val_loss: 10.1826 - val_acc: 0.3682\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 88s 1ms/step - loss: 10.2322 - acc: 0.3652 - val_loss: 10.4606 - val_acc: 0.3510\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 80s 1ms/step - loss: 10.4837 - acc: 0.3496 - val_loss: 10.3269 - val_acc: 0.3593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADoBJREFUeJzt3X2MXOV1x/HfyXq9jo1JvHHYboiLHeMEiGlMOjIgLKCiuA5CMiiKiRVFDiFxmuCktK4EdavGrWjlVgmRQynS0ri2I95CAsJ/0CR0FUGiwpbFMeYtvJlNY7PsYjZgQ4i9Xp/+sdfRBnaeWc/cmTu75/uRVjtzz71zj6792zszz8x9zN0FIJ53Fd0AgGIQfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQU1r5M6mW5vP0KxG7hII5bd6U4f9kE1k3ZrCb2YrJG2W1CLpP9x9U2r9GZqls+2iWnYJIKHHuye8btVP+82sRdJNkj4h6QxJq83sjGofD0Bj1fKaf6mk5919j7sflnSHpJX5tAWg3moJ/8mSfjXm/t5s2e8xs7Vm1mtmvcM6VMPuAOSp7u/2u3uXu5fcvdSqtnrvDsAE1RL+fZLmjbn/wWwZgEmglvA/ImmRmS0ws+mSPi1pRz5tAai3qof63P2Ima2T9CONDvVtcfcnc+sMQF3VNM7v7vdJui+nXgA0EB/vBYIi/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+IKiaZuk1sz5JByWNSDri7qU8mkJ+bFr6n7jl/XPruv9n/np+2drIzKPJbU9ZOJisz/yKJesv3zC9bG1n6c7ktvtH3kzWz75rfbJ+6l89nKw3g5rCn/kTd9+fw+MAaCCe9gNB1Rp+l/RjM3vUzNbm0RCAxqj1af8yd99nZidJut/MfuHuD45dIfujsFaSZmhmjbsDkJeazvzuvi/7PSjpHklLx1mny91L7l5qVVstuwOQo6rDb2azzGz2sduSlkt6Iq/GANRXLU/7OyTdY2bHHuc2d/9hLl0BqLuqw+/ueyR9LMdepqyW0xcl697Wmqy/dMF7k/W3zik/Jt3+nvR49U8/lh7vLtJ//WZ2sv4v/7YiWe8587aytReH30puu2ng4mT9Az/1ZH0yYKgPCIrwA0ERfiAowg8ERfiBoAg/EFQe3+oLb+TCjyfrN2y9KVn/cGv5r55OZcM+kqz//Y2fS9anvZkebjv3rnVla7P3HUlu27Y/PRQ4s7cnWZ8MOPMDQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCM8+eg7ZmXkvVHfzsvWf9w60Ce7eRqff85yfqeN9KX/t668Ptla68fTY/Td3z7f5L1epr8X9itjDM/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRl7o0b0TzR2v1su6hh+2sWQ1eem6wfWJG+vHbL7hOS9ce+cuNx93TM9fv/KFl/5IL0OP7Ia68n635u+au7930tuakWrH4svQLeoce7dcCH0nOXZzjzA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQFcf5zWyLpEslDbr74mxZu6Q7Jc2X1Cdplbv/utLOoo7zV9Iy933J+sirQ8n6i7eVH6t/8vwtyW2X/vNXk/WTbiruO/U4fnmP82+V9PaJ0K+T1O3uiyR1Z/cBTCIVw+/uD0p6+6lnpaRt2e1tki7LuS8AdVbta/4Od+/Pbr8sqSOnfgA0SM1v+PnomwZl3zgws7Vm1mtmvcM6VOvuAOSk2vAPmFmnJGW/B8ut6O5d7l5y91Kr2qrcHYC8VRv+HZLWZLfXSLo3n3YANErF8JvZ7ZIekvQRM9trZldJ2iTpYjN7TtKfZvcBTCIVr9vv7qvLlBiwz8nI/ldr2n74wPSqt/3oZ55K1l+5uSX9AEdHqt43isUn/ICgCD8QFOEHgiL8QFCEHwiK8ANBMUX3FHD6tc+WrV15ZnpE9j9P6U7WL/jU1cn67DsfTtbRvDjzA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQjPNPAalpsl/98unJbf9vx1vJ+nXXb0/W/2bV5cm6//w9ZWvz/umh5LZq4PTxEXHmB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgKk7RnSem6G4+Q58/N1m/9evfSNYXTJtR9b4/un1dsr7olv5k/cievqr3PVXlPUU3gCmI8ANBEX4gKMIPBEX4gaAIPxAU4QeCqjjOb2ZbJF0qadDdF2fLNkr6oqRXstU2uPt9lXbGOP/k4+ctSdZP3LQ3Wb/9Qz+qet+n/eQLyfpH/qH8dQwkaeS5PVXve7LKe5x/q6QV4yz/lrsvyX4qBh9Ac6kYfnd/UNJQA3oB0EC1vOZfZ2a7zWyLmc3JrSMADVFt+G+WtFDSEkn9kr5ZbkUzW2tmvWbWO6xDVe4OQN6qCr+7D7j7iLsflXSLpKWJdbvcveTupVa1VdsngJxVFX4z6xxz93JJT+TTDoBGqXjpbjO7XdKFkuaa2V5JX5d0oZktkeSS+iR9qY49AqgDvs+PmrR0nJSsv3TFqWVrPdduTm77rgpPTD/z4vJk/fVlrybrUxHf5wdQEeEHgiL8QFCEHwiK8ANBEX4gKIb6UJjv7U1P0T3Tpifrv/HDyfqlX72m/GPf05PcdrJiqA9ARYQfCIrwA0ERfiAowg8ERfiBoAg/EFTF7/MjtqPL0pfufuFT6Sm6Fy/pK1urNI5fyY1DZyXrM+/trenxpzrO/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOP8U5yVFifrz34tPdZ+y3nbkvXzZ6S/U1+LQz6crD88tCD9AEf7c+xm6uHMDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBVRznN7N5krZL6pDkkrrcfbOZtUu6U9J8SX2SVrn7r+vXalzTFpySrL9w5QfK1jZecUdy20+esL+qnvKwYaCUrD+w+Zxkfc629HX/kTaRM/8RSevd/QxJ50i62szOkHSdpG53XySpO7sPYJKoGH5373f3ndntg5KelnSypJWSjn38a5uky+rVJID8HddrfjObL+ksST2SOtz92OcnX9boywIAk8SEw29mJ0j6gaRr3P3A2JqPTvg37qR/ZrbWzHrNrHdYh2pqFkB+JhR+M2vVaPBvdfe7s8UDZtaZ1TslDY63rbt3uXvJ3UutasujZwA5qBh+MzNJ35H0tLvfMKa0Q9Ka7PYaSffm3x6AepnIV3rPk/RZSY+b2a5s2QZJmyR9z8yukvRLSavq0+LkN23+Hybrr/9xZ7J+xT/+MFn/8/fenazX0/r+9HDcQ/9efjivfev/Jredc5ShvHqqGH53/5mkcvN9X5RvOwAahU/4AUERfiAowg8ERfiBoAg/EBThB4Li0t0TNK3zD8rWhrbMSm775QUPJOurZw9U1VMe1u1blqzvvDk9Rffc7z+RrLcfZKy+WXHmB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgwozzH/6z9GWiD//lULK+4dT7ytaWv/vNqnrKy8DIW2Vr5+9Yn9z2tL/7RbLe/lp6nP5osopmxpkfCIrwA0ERfiAowg8ERfiBoAg/EBThB4IKM87fd1n679yzZ95Vt33f9NrCZH3zA8uTdRspd+X0Uadd/2LZ2qKBnuS2I8kqpjLO/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QlLl7egWzeZK2S+qQ5JK63H2zmW2U9EVJr2SrbnD38l96l3SitfvZxqzeQL30eLcO+FD6gyGZiXzI54ik9e6+08xmS3rUzO7Pat9y929U2yiA4lQMv7v3S+rPbh80s6clnVzvxgDU13G95jez+ZLOknTsM6PrzGy3mW0xszlltllrZr1m1jusQzU1CyA/Ew6/mZ0g6QeSrnH3A5JulrRQ0hKNPjP45njbuXuXu5fcvdSqthxaBpCHCYXfzFo1Gvxb3f1uSXL3AXcfcfejkm6RtLR+bQLIW8Xwm5lJ+o6kp939hjHLO8esdrmk9HStAJrKRN7tP0/SZyU9bma7smUbJK02syUaHf7rk/SlunQIoC4m8m7/zySNN26YHNMH0Nz4hB8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiCoipfuznVnZq9I+uWYRXMl7W9YA8enWXtr1r4keqtWnr2d4u7vn8iKDQ3/O3Zu1uvupcIaSGjW3pq1L4neqlVUbzztB4Ii/EBQRYe/q+D9pzRrb83al0Rv1Sqkt0Jf8wMoTtFnfgAFKST8ZrbCzJ4xs+fN7LoieijHzPrM7HEz22VmvQX3ssXMBs3siTHL2s3sfjN7Lvs97jRpBfW20cz2Zcdul5ldUlBv88zsJ2b2lJk9aWZ/kS0v9Ngl+irkuDX8ab+ZtUh6VtLFkvZKekTSand/qqGNlGFmfZJK7l74mLCZnS/pDUnb3X1xtuxfJQ25+6bsD+ccd7+2SXrbKOmNomduziaU6Rw7s7SkyyR9TgUeu0Rfq1TAcSvizL9U0vPuvsfdD0u6Q9LKAvpoeu7+oKShty1eKWlbdnubRv/zNFyZ3pqCu/e7+87s9kFJx2aWLvTYJfoqRBHhP1nSr8bc36vmmvLbJf3YzB41s7VFNzOOjmzadEl6WVJHkc2Mo+LMzY30tpmlm+bYVTPjdd54w++dlrn7xyV9QtLV2dPbpuSjr9maabhmQjM3N8o4M0v/TpHHrtoZr/NWRPj3SZo35v4Hs2VNwd33Zb8HJd2j5pt9eODYJKnZ78GC+/mdZpq5ebyZpdUEx66ZZrwuIvyPSFpkZgvMbLqkT0vaUUAf72Bms7I3YmRmsyQtV/PNPrxD0prs9hpJ9xbYy+9plpmby80srYKPXdPNeO3uDf+RdIlG3/F/QdLfFtFDmb4+JOmx7OfJonuTdLtGnwYOa/S9kaskvU9St6TnJP23pPYm6u27kh6XtFujQessqLdlGn1Kv1vSruznkqKPXaKvQo4bn/ADguINPyAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQf0/sEWOix6VKakAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "#download mnist data and split into train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#plot the first image in the dataset\n",
    "plt.imshow(X_train[0])\n",
    "\n",
    "#reshape data to fit model\n",
    "X_train = X_train.reshape(60000,28,28,1)\n",
    "X_test = X_test.reshape(10000,28,28,1)\n",
    "from keras.utils import to_categorical\n",
    "#one-hot encode target column\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "#create model\n",
    "model = Sequential()\n",
    "#add model layers\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "#compile model using accuracy to measure model performance\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10)\n",
    "\n",
    "#predict first 4 images in the test set\n",
    "model.predict(X_test[:4])\n",
    "\n",
    "#actual results for first 4 images in test set\n",
    "y_test[:4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
